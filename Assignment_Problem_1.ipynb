{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eU9Ov_hp0BE"
      },
      "source": [
        "I have mounted the google drive and mentioned the path where the dataset is present"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIOI77AZSwYQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7W-d3yuTDZK"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/Assignment/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMZ6ryB0z8bD"
      },
      "source": [
        "I have installed Augmentor so that I can fetch augmentation modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wELpolrWTD4I"
      },
      "outputs": [],
      "source": [
        "!pip install Augmentor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4XQlyp9qGgC"
      },
      "source": [
        "I have installed pytorch-metric-learning for required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Skvd4TyTFuE"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-metric-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aimSfNexqfGs"
      },
      "source": [
        "I have loaded the dataset and implemented augmentation to increase the dataset size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L9h1BOfTTHvP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import Augmentor\n",
        "torch.manual_seed(1)\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class SkinLessionDataset(Dataset):\n",
        "    def __init__(self, csv_path, image_dir, transform = None):\n",
        "        df = pd.read_csv(csv_path)\n",
        "        self.image_dir = image_dir\n",
        "        self.image_names = df[\"image_train\"]\n",
        "        self.image_labels = df[\"label_train\"]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = Image.open(os.path.join(self.image_dir, self.image_names[index]))\n",
        "\n",
        "        p = Augmentor.DataPipeline([[np.array(image)]])\n",
        "        p.flip_random(probability=0.7)\n",
        "        p.rotate(probability=0.7, max_left_rotation=25, max_right_rotation=25)\n",
        "        p.rotate_without_crop(probability=0.7, max_left_rotation=10, max_right_rotation=10, expand=True)\n",
        "        p.zoom(probability=0.7, min_factor=1.1, max_factor=1.7)\n",
        "        p.random_distortion(probability=0.7, grid_width=10, grid_height=10, magnitude=20)\n",
        "        p.scale(probability=0.6, scale_factor=1.3)\n",
        "        p.random_brightness(probability=0.5, min_factor=0.35, max_factor=0.65)\n",
        "        p.crop_random(probability=0.7, percentage_area=0.5)\n",
        "\n",
        "\n",
        "        p.resize(probability=1.0, width=224, height=224)\n",
        "\n",
        "\n",
        "        image_aug = p.sample(1)\n",
        "\n",
        "        image = Image.fromarray(image_aug[0][0])\n",
        "        # print(image)\n",
        "\n",
        "        if not self.transform is None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, self.image_labels[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.image_labels.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbCp0GjkqrCZ"
      },
      "source": [
        "Defined class for contrastive loss and performed normalization and transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bxvqrcOBTMdO"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from pytorch_metric_learning import losses\n",
        "\n",
        "class SupervisedContrastiveLoss(nn.Module):\n",
        "  def __init__(self, temperature=0.1):\n",
        "    super(SupervisedContrastiveLoss, self).__init__()\n",
        "    self.temperature = temperature\n",
        "\n",
        "  def forward(self, feature_vectors, labels):\n",
        "    # Normalize feature vectors\n",
        "    feature_vectors_normalized = F.normalize(feature_vectors, p=2, dim=1)\n",
        "    # Compute logits\n",
        "    logits = torch.div(\n",
        "        torch.matmul(\n",
        "            feature_vectors_normalized, torch.transpose(feature_vectors_normalized, 0, 1)\n",
        "        ),\n",
        "        self.temperature,\n",
        "    )\n",
        "    return losses.NTXentLoss(temperature=0.07)(logits, torch.squeeze(labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpFljoZGrLmi"
      },
      "source": [
        "Loaded the train and test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y8Dx7ApATPu8"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "bs = 16\n",
        "\n",
        "skin_lession_transform_train = transforms.Compose([\n",
        "                                                # transforms.Resize(224),\n",
        "                                                transforms.ToTensor(),\n",
        "                                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "                                          ])\n",
        "\n",
        "\n",
        "skin_lession_transform_test = transforms.Compose([\n",
        "                                                # transforms.Resize(224),\n",
        "                                                transforms.ToTensor(),\n",
        "                                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "                                          ])\n",
        "\n",
        "train_dataset = SkinLessionDataset(\n",
        "                                    csv_path = \"/content/drive/MyDrive/Assignment/train_final.csv\",\n",
        "                                    image_dir = \"/content/drive/MyDrive/Assignment/train_images/\",\n",
        "                                    transform = skin_lession_transform_train\n",
        "                                  )\n",
        "\n",
        "train_loader = DataLoader(\n",
        "                            dataset = train_dataset,\n",
        "                            batch_size = bs,\n",
        "                            drop_last = True,\n",
        "                            shuffle = True,\n",
        "                            num_workers = 2\n",
        "                        )\n",
        "\n",
        "test_dataset = SkinLessionDataset(\n",
        "                                    csv_path = \"/content/drive/MyDrive/Assignment/test.csv\",\n",
        "                                    image_dir = \"/content/drive/MyDrive/Assignment/test_images/\",\n",
        "                                    transform = skin_lession_transform_test\n",
        "                                )\n",
        "test_loader = DataLoader(\n",
        "                            dataset = test_dataset,\n",
        "                            batch_size = bs,\n",
        "                            drop_last = True,\n",
        "                            shuffle = False,\n",
        "                            num_workers = 2\n",
        "                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXB_EeemrP2p"
      },
      "source": [
        "GPU allocation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giE7GnT4TT_1"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "# from pytorch_metric_learning import losses\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn2CQXNvrgKr"
      },
      "source": [
        "Defined the operations to be performed on the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_H7WjzATX0Y"
      },
      "outputs": [],
      "source": [
        "!pip install -U scikit-learn scipy matplotlib\n",
        "!pip install sklearn\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "def trainer(model, dataloaders, criterion, optimizer, scheduler, eval = True, num_epochs=10):\n",
        "  since = time.time()\n",
        "\n",
        "  val_acc_history = []\n",
        "  train_losses = []\n",
        "  train_accuracy = []\n",
        "  test_losses = []\n",
        "  test_accuracy = []\n",
        "\n",
        "  best_model_wts = None\n",
        "\n",
        "  best_acc = float('-inf')\n",
        "\n",
        "  writer = SummaryWriter()\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_corrects = 0\n",
        "\n",
        "    # iterate over the dataset\n",
        "    train_iterator = tqdm(enumerate(dataloaders['train']),\n",
        "                          'Training',\n",
        "                          total=len(dataloaders['train']))\n",
        "    for i, (images, labels) in train_iterator:\n",
        "\n",
        "      # images and labels to the device\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # zero the optimizer gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # One forward pass\n",
        "      outputs = model(images)\n",
        "\n",
        "      # calculate loss\n",
        "      if (len(criterion)==1):\n",
        "        loss = criterion[0](outputs, labels)\n",
        "      elif (len(criterion)==2):\n",
        "        loss = criterion[0](outputs, labels) + criterion[1](outputs, labels)\n",
        "      loss.backward() # one backward pass\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      optimizer.step() # update the optimizer parameters\n",
        "\n",
        "      # get the output\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "\n",
        "      # statistics\n",
        "      running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    # train loss and accuracy\n",
        "    train_loss = running_loss / len(dataloaders['train'].dataset)\n",
        "    train_acc = running_corrects.double() / len(dataloaders['train'].dataset)\n",
        "\n",
        "    print('{} Loss: {:.4f} Acc: {:.4f}'.format('train', train_loss, train_acc))\n",
        "\n",
        "    writer.add_scalar('Loss/train', train_loss,epoch)\n",
        "    writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
        "\n",
        "    if eval:\n",
        "      # set the model evaluation mode\n",
        "      model.eval()\n",
        "\n",
        "      with torch.no_grad():\n",
        "        test_correct = 0\n",
        "        running_loss = 0\n",
        "\n",
        "        test_iterator = tqdm(enumerate(dataloaders['val']),\n",
        "                             'Validation',\n",
        "                             total=len(dataloaders['val']))\n",
        "        for j, (images,labels) in test_iterator:\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          test_outputs = model(images)\n",
        "\n",
        "          if (len(criterion)==1):\n",
        "            loss = criterion[0](outputs, labels)\n",
        "          elif (len(criterion)==2):\n",
        "            loss = criterion[0](outputs, labels) + criterion[1](outputs, labels)\n",
        "\n",
        "          running_loss += loss.item()\n",
        "          _, test_predicted = torch.max(test_outputs, 1)\n",
        "\n",
        "          test_correct += torch.sum(test_predicted == labels.data)\n",
        "          pred=torch.tensor(test_predicted).detach().cpu().numpy()\n",
        "          labeltest=torch.tensor(labels.data).detach().cpu().numpy()\n",
        "          cm=metrics.confusion_matrix(pred,labeltest)\n",
        "\n",
        "\n",
        "\n",
        "      test_loss = running_loss / len(dataloaders['val'].dataset)\n",
        "      test_acc = test_correct.double() / len(dataloaders['val'].dataset)\n",
        "      plt.figure(figsize=(9,9))\n",
        "      sns.heatmap(cm,annot=True,fmt='0.3f',linewidth=0.5,square=True,cbar=False)\n",
        "      #plt.ylabel('actual values')\n",
        "      #plt.xlabel('predicted values')\n",
        "      #plt.show()\n",
        "      print(metrics.classification_report(pred,labeltest))\n",
        "      print('{} Loss: {:.4f} Acc: {:.4f}'.format('val', test_loss, test_acc))\n",
        "\n",
        "      writer.add_scalar('Loss/test', test_loss, epoch)\n",
        "      writer.add_scalar('Accuracy/test',test_acc, epoch)\n",
        "\n",
        "      # deep copy the model\n",
        "      if train_acc > best_acc:\n",
        "        best_acc = train_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    val_acc_history.append(test_acc)\n",
        "    train_accuracy.append(train_acc)\n",
        "    train_losses.append(train_loss)\n",
        "    test_accuracy.append(test_acc)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "\n",
        "    # scheduler.step()\n",
        "  time_elapsed = time.time() - since\n",
        "  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "  print('Best val Acc: {:4f}'.format(best_acc))\n",
        "  #print(metrics.classification_report(pred,labeltest))\n",
        "\n",
        "  Path('model_store/').mkdir(parents=True, exist_ok=True)\n",
        "  if (len(criterion)==1):\n",
        "    torch.save(best_model_wts, 'model_store/'+'resnet50-best-model-parameters_without_scl.pth')\n",
        "  elif (len(criterion)==2):\n",
        "    torch.save(best_model_wts, 'model_store/'+'resnet50-best-model-parameters_with_scl.pth')\n",
        "  return model, train_accuracy, train_losses, test_accuracy, test_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4JOn9T5OTd-q"
      },
      "outputs": [],
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "  if feature_extracting:\n",
        "    for param in model.parameters():\n",
        "      param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRBCslG0r3T-"
      },
      "source": [
        "Mentioned number of classes, number of epochs and feature extract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nnfVNM7YTh2R"
      },
      "outputs": [],
      "source": [
        "# Number of classes in the dataset\n",
        "num_classes = 7\n",
        "\n",
        "\n",
        "\n",
        "# Number of epochs to train for\n",
        "num_epochs = 10\n",
        "\n",
        "\n",
        "#   Feature extract is True for updating the reshaped layer params\n",
        "feature_extract = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLXFaG5wsISA"
      },
      "source": [
        "Allocating the Resnet50 model and mentioned the image size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAAQmvYnTkW-"
      },
      "outputs": [],
      "source": [
        "model_ft = models.resnet50(pretrained=True)\n",
        "set_parameter_requires_grad(model_ft, feature_extract)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "input_size = 224\n",
        "\n",
        "model_ft = model_ft.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heosFLCIsTkx"
      },
      "source": [
        "Defined different types of optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxBPzxOwTmoD"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "#optimizer_ft = optim.SGD(params_to_update, lr=0.004, momentum=0.9)\n",
        "optimizer_ft = optim.Adam(params_to_update, lr=0.001)\n",
        "#optimizer_ft = optim.RMSprop(params_to_update, lr=0.004)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL16z_Jqsgsp"
      },
      "source": [
        "Running the model and evaluating the loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFoHUDL-TpMw"
      },
      "outputs": [],
      "source": [
        "# Setup the loss fxn\n",
        "criterion = [nn.CrossEntropyLoss()]\n",
        "\n",
        "# Train and evaluate\n",
        "model, train_accuracy, train_losses, test_accuracy, test_losses = trainer(\n",
        "                          model_ft,\n",
        "                          {\"train\" : train_loader, \"val\" : test_loader},\n",
        "                          criterion,\n",
        "                          optimizer_ft, scheduler, num_epochs=num_epochs\n",
        "                            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFM65XnRwbCo"
      },
      "source": [
        "Evaluating the loss and accuracy adding Contrastive loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgJqN066TrUG"
      },
      "outputs": [],
      "source": [
        "# Setup the loss fxn\n",
        "criterion = [nn.CrossEntropyLoss(), SupervisedContrastiveLoss()]\n",
        "\n",
        "# Train and evaluate\n",
        "model, train_accuracy, train_losses, test_accuracy, test_losses = trainer(\n",
        "                          model_ft,\n",
        "                          {\"train\" : train_loader, \"val\" : test_loader},\n",
        "                          criterion,\n",
        "                          optimizer_ft, scheduler, num_epochs=num_epochs\n",
        "                            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GlexrNu4ANI"
      },
      "source": [
        "Plotting the graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJlkwWGY-Pkl"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_losses, label='training loss')\n",
        "plt.plot(test_losses, label='validation loss')\n",
        "plt.title('Loss at the end of each epoch')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting tensor list to numpy array"
      ],
      "metadata": {
        "id": "OKiSDezqj3eO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxzGoiadK9Ih"
      },
      "outputs": [],
      "source": [
        "test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XGBfjNT9QpHY"
      },
      "outputs": [],
      "source": [
        "t=torch.tensor(test_accuracy).detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pC-c9f0UQyGm"
      },
      "outputs": [],
      "source": [
        "d=torch.tensor(train_accuracy).detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tsqmr0tNQ3Nt"
      },
      "outputs": [],
      "source": [
        "plt.plot(d, label='training accuracy')\n",
        "plt.plot(t, label='validation accuracy')\n",
        "plt.title('Accuracy at the end of each epoch')\n",
        "plt.legend();"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}